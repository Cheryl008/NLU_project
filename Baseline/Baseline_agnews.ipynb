{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as9Sy3wUJFAU"
   },
   "source": [
    "# **Install Raw Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OtybZIsLJOqK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KuNK8996QocH"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"agNews_train.csv\",header=0,names=['id','categories','text'])\n",
    "test_data=pd.read_csv(\"agNews_test.csv\",header=0,names=['id','categories','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1650320415835,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "HPl4yprWS8rh",
    "outputId": "45bb39cd-c000-449f-ec28-6616ef60c3ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Iran Says Its Missiles Can Now Reach 1,250 Mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Italian women kidnapped in raid on relief agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Insurgent Alliance  Is Fraying  in Fallujah Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Rahul the  #39;darling #39; at AICC conclave N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Judges Postpone Milosevic Trial for Month (AP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  categories                                               text\n",
       "0   0           0  Iran Says Its Missiles Can Now Reach 1,250 Mil...\n",
       "1   1           0  Italian women kidnapped in raid on relief agen...\n",
       "2   2           0  Insurgent Alliance  Is Fraying  in Fallujah Re...\n",
       "3   3           0  Rahul the  #39;darling #39; at AICC conclave N...\n",
       "4   4           0  Judges Postpone Milosevic Trial for Month (AP)..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['categories'] = train_data['categories'].map({1:0,2:1,3:2,4:3})\n",
    "test_data['categories'] = test_data['categories'].map({1:0,2:1,3:2,4:3})\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "2    1000\n",
       "3    1000\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['categories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    400\n",
       "2    400\n",
       "3    400\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['categories'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z_ljTciWqFb"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1650320422067,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "7zPQUDsKXdyd",
    "outputId": "e88e268a-be47-40de-a9f8-c5eb96ac4a60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fengwenxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GeJdGdXoXip0"
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords_1 = stopwords + [\"dont\"]\n",
    "\n",
    "keep_words = ['of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'no', 'nor', 'not', 'be', 'do', 'can', 'have']\n",
    "\n",
    "stopwords_2 = []\n",
    "for stop in stopwords:\n",
    "    if stop not in keep_words:\n",
    "        stopwords_2.append(stop)\n",
    "\n",
    "raw_train_text_list = train_data['text'].values.tolist()\n",
    "raw_test_text_list = test_data['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Olw7ocCnYpeH"
   },
   "outputs": [],
   "source": [
    "def is_num(num):\n",
    "    pattern = re.compile(r'[-+]?[0-9\\.,]*[0-9][a-zA-Z]*$')\n",
    "    result = pattern.match(num)\n",
    "    return result\n",
    "\n",
    "def is_be(word):\n",
    "\tpattern = ['be', 'is', 'are', 'am', 'was', 'were', 'being', 'been']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_do(word):\n",
    "\tpattern = ['do', 'does', 'did', 'doing', 'done']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_dont(word):\n",
    "    pattern = [\"don't\", \"doesn't\"]\n",
    "    return word in pattern\n",
    "\n",
    "def is_cant(word):\n",
    "    pattern = [\"cannot\", \"can't\"]\n",
    "    return word in pattern\n",
    "\n",
    "def is_have(word):\n",
    "\tpattern = ['have', 'has', 'had', 'having']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_will(word):\n",
    "\tpattern = ['shall', 'should', 'will', 'would', 'd', 'll']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_get(word):\n",
    "\tpattern = ['get', 'gets', 'got', 'gotten', 'getting']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_seem(word):\n",
    "\tpattern = ['seem', 'seeming', 'seems', 'seemed']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_a(word):\n",
    "\tpattern = ['a', 'the', 'an', 'this', 'that']\n",
    "\treturn word in pattern\n",
    "\n",
    "def is_good(word):\n",
    "    pattern = ['good', 'better', 'best']\n",
    "    return word in pattern\n",
    "\n",
    "def is_bad(word):\n",
    "    pattern = ['bad', 'worse', 'worst']\n",
    "    return word in pattern\n",
    "\n",
    "def trans_num(title):\n",
    "    title_list = title.split(\" \")\n",
    "    new_title = []\n",
    "    for word in title_list:\n",
    "        if (is_num(word)):\n",
    "            word = \"isnum\"\n",
    "        new_title.append(word)\n",
    "    new_title = \" \".join(new_title)\n",
    "    return new_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ymn-VXi_ao8K"
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_title):\n",
    "    raw_lists = raw_title.split(\" \")\n",
    "    title = []\n",
    "  \n",
    "    for raw in raw_lists:\n",
    "        #1. Case Folding\n",
    "        raw = str(raw.lower())\n",
    "\t\t#2. Lab Numbers\n",
    "        new_raw = trans_num(raw)\n",
    "\t\t#3. Remove Punctuations\n",
    "        letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", new_raw)\n",
    "        word_1 = \" \".join(letters_only.split())\n",
    "        words = word_1.split(\" \")\n",
    "\t \t#4. Stemming & Lemmatization\n",
    "        for word in words:\n",
    "            if (word == \"\"):\n",
    "                word = \"\"\n",
    "            elif (is_be(word)):\n",
    "                word = \"be\"\n",
    "            elif (is_do(word)):\n",
    "                word = \"do\"\n",
    "            elif (is_dont(word)):\n",
    "                word = \"do not\"\n",
    "            elif (is_cant(word)):\n",
    "                word = \"can not\"\n",
    "            elif (is_have(word)):\n",
    "                word = \"have\"\n",
    "            elif (is_will(word)):\n",
    "                word = \"will\"\n",
    "            elif (is_get(word)):\n",
    "                word = \"get\"\n",
    "            elif (is_seem(word)):\n",
    "                word = \"seem\"\n",
    "            elif (is_a(word)):\n",
    "                word = \"a\"\n",
    "            elif (is_num(word)):\n",
    "                word = \"is_num\"\n",
    "            elif (is_good(word)):\n",
    "                word = \"good\"\n",
    "            elif (is_bad(word)):\n",
    "                word = \"bad\"\n",
    "            elif (word[-3:] == \"ing\") & (len(word) > 5):\n",
    "                if word[-4] == word[-5]:\n",
    "                    word = word[:-4]\n",
    "                elif word[-5] in ['a', 'e', 'i', 'o', 'u']:\n",
    "                    word = word[:-3] + 'e'\n",
    "                else:\n",
    "                    word = word[:-3]\n",
    "            elif(word[-3:] == \"ies\"):\n",
    "                word = word[:-3] + \"y\"\n",
    "            elif (word[-2:] == \"ed\") & (len(word) > 5):\n",
    "                word = word[:-2]\n",
    "            elif (word[-1] == \"s\") & (len(word) > 3):\n",
    "                word = word[:-1]\n",
    "            if word != '':\n",
    "                title.append(word)\n",
    "    title = \" \".join(title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for text in raw_train_text_list:\n",
    "    train_list.append(preprocess(text))\n",
    "    \n",
    "for text in raw_test_text_list:\n",
    "    test_list.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iran say its missile can now reach isnum mile tehran reuter iran have increas a range of its missile to isnum mile a senior official be quot as saye on tuesday'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yYNeNMDZri1"
   },
   "source": [
    "# **Word Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "X_train = vectorizer.fit_transform(train_list).toarray()\n",
    "X_test = vectorizer.fit_transform(test_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 5000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB7_pyEClLeE"
   },
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_metrics(true_label,preds):  \n",
    "    from sklearn.metrics import accuracy_score, normalized_mutual_info_score, adjusted_rand_score, mutual_info_score, adjusted_mutual_info_score\n",
    "    ACC = round(accuracy_score(true_label,preds),3)\n",
    "    if ACC<=1/len(true_label.unique()): #说明：聚类的label，和真实的label没对上（改好了，不需要调整）\n",
    "        keys = list(pd.value_counts(preds).index)\n",
    "        values = list(pd.value_counts(true_label).index)\n",
    "        dic = dict(zip(keys, values))\n",
    "        preds = pd.Series(preds).map(dic)\n",
    "    NMI = round(normalized_mutual_info_score(true_label.to_numpy(),preds),3)\n",
    "    ARI = round(adjusted_rand_score(true_label.to_numpy(),preds),3)\n",
    "    ACC = round(accuracy_score(true_label,preds),3)\n",
    "    # MI = mutual_info_score(true_label,preds)\n",
    "    return {'ACC':ACC,'NMI':NMI,'ARI':ARI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = train_data['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC8mnVLHlSW4"
   },
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline K-means {'ACC': 0.412, 'NMI': 0.123, 'ARI': 0.062}\n"
     ]
    }
   ],
   "source": [
    "#K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "clustering_model = KMeans(n_clusters = 4, \n",
    "                          init = 'k-means++',\n",
    "                          max_iter = 300, n_init = 10,random_state=123)\n",
    "clustering_model.fit(X_train)\n",
    "KMeans_label = clustering_model.predict(X_train)\n",
    "print('Baseline K-means',three_metrics(true_label,KMeans_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ztDrQE7xybQ"
   },
   "source": [
    "### Fuzzy C Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline FuzzyC {'ACC': 0.286, 'NMI': 0.13, 'ARI': 0.097}\n"
     ]
    }
   ],
   "source": [
    "#FCM\n",
    "from fcmeans import FCM\n",
    "fcm = FCM(n_clusters=4)\n",
    "fcm.fit(X_train)\n",
    "FCM_label = fcm.predict(X_train)\n",
    "print('Baseline FuzzyC',three_metrics(true_label,FCM_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMkexO4fz1g7"
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train2 = scaler.fit_transform(X_train)\n",
    "X_test2 = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LDA {'ACC': 0.208, 'NMI': 0.009, 'ARI': 0.009}\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4, random_state=456)\n",
    "lda.fit(X_test2)\n",
    "doc_topic_dist_unnormalized = np.matrix(lda.transform(X_train2))\n",
    "doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
    "LDA_label = list(np.array(doc_topic_dist.argmax(axis=1)).T[0])\n",
    "print('Baseline LDA',three_metrics(true_label,LDA_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4BOtrUpBFk_"
   },
   "source": [
    "### Deep Embedded Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vCPMf53LI9X5"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "maxlen = 4096 #only use this number of most frequent words\n",
    "training_samples = 8000\n",
    "validation_samples = 4500\n",
    "max_words = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1650327352853,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "hsp4j4twI_au",
    "outputId": "93a98a59-3809-45c7-cd3a-455bb005bbed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4527x13617 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 214152 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_words)\n",
    "\n",
    "numeric_columns = train_data.columns.values.tolist()\n",
    "\n",
    "train = vectorizer.fit_transform(train_list)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1650327354062,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "JxN9B0M3JoTH",
    "outputId": "d868726f-6e66-43ad-d199-aa1c2f80e49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13642 unique tokens found\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_list) # generates word index\n",
    "sequences = tokenizer.texts_to_sequences(train_list) # transforms strings in list of intergers\n",
    "word_index = tokenizer.word_index # calculated word index\n",
    "print(f\"{len(word_index)} unique tokens found\")\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen) #transforms integer lists into 2D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "1NrIlRMWK0XG"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "data_1 = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1650327357493,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "8zu9xbvWK1c6",
    "outputId": "df0af9af-79a0-4ea3-cc5e-7cc80516bcc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.43920413e-01, 5.97550045e-04, 1.15660107e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.47383935e-04, 1.26979385e-03, 2.78971575e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        3.90567428e-03, 1.44980580e-01, 4.52386338e-04],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        4.49521002e-03, 4.18285031e-03, 1.65121013e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        3.68459838e-04, 3.65999402e-03, 3.01590892e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        3.68459838e-04, 0.00000000e+00, 3.01590892e-04]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "rb0cY5VFBJVp"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "zc4yOaXSGSQ4"
   },
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected symmetric auto-encoder model.\n",
    "  \n",
    "    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n",
    "          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n",
    "\n",
    "    act: activation function\n",
    "    \n",
    "    return:\n",
    "        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    \n",
    "    input_data = Input(shape=(dims[0],), name='input')\n",
    "    x = input_data\n",
    "    \n",
    "    # internal layers of encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # latent hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers of decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # decoder output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n",
    "    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n",
    "    \n",
    "    return autoencoder_model, encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "KZv3CZxaGV_t"
   },
   "outputs": [],
   "source": [
    "n_clusters = 2 \n",
    "n_epochs   = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650327365311,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "-CcUx2L-GY2F",
    "outputId": "27902a69-32a8-491f-a24f-2073a316e548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dims = [x.shape[-1], 500, 500, 2000, 10] \n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                           distribution='uniform')\n",
    "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
    "pretrain_epochs = n_epochs\n",
    "batch_size = batch_size\n",
    "# save_dir = 'kaggle/working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1650327366984,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "mApfiIpNGir7",
    "outputId": "4a4cb659-926a-46c8-e1dc-4c0212506c1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4096, 500, 500, 2000, 10]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "9gRR3yxuGlb0"
   },
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    '''\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        ''' \n",
    "        student t-distribution, as used in t-SNE algorithm.\n",
    "        It measures the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "       \n",
    "        inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        \n",
    "        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        '''\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
    "        \n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "5DLN38W1GriZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 05:15:40.636050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3457,
     "status": "ok",
     "timestamp": 1650327382066,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "1Q7s-hcNHAVN",
    "outputId": "aaebf540-c1dc-4a08-9d1e-11ac715b9b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "36/36 [==============================] - 2s 38ms/step - loss: 4.2185e-04\n",
      "Epoch 2/15\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 4.1618e-04\n",
      "Epoch 3/15\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 4.1213e-04\n",
      "Epoch 4/15\n",
      "36/36 [==============================] - 2s 44ms/step - loss: 4.0940e-04\n",
      "Epoch 5/15\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 4.0753e-04\n",
      "Epoch 6/15\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 4.0625e-04\n",
      "Epoch 7/15\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 4.0536e-04\n",
      "Epoch 8/15\n",
      "36/36 [==============================] - 2s 46ms/step - loss: 4.0474e-04\n",
      "Epoch 9/15\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 4.0432e-04\n",
      "Epoch 10/15\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 4.0403e-04\n",
      "Epoch 11/15\n",
      "36/36 [==============================] - 2s 44ms/step - loss: 4.0383e-04\n",
      "Epoch 12/15\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 4.0369e-04\n",
      "Epoch 13/15\n",
      "36/36 [==============================] - 2s 44ms/step - loss: 4.0359e-04\n",
      "Epoch 14/15\n",
      "36/36 [==============================] - 2s 48ms/step - loss: 4.0353e-04\n",
      "Epoch 15/15\n",
      "36/36 [==============================] - 2s 53ms/step - loss: 4.0348e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153465040>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "VYk7ycjbHOhA"
   },
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "LJsr_EZUHZZD"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1650327388129,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "utoBH-N6IqFV",
    "outputId": "573e8f9f-44c2-4a93-de75-2179f012b0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.09156133e-05, -1.03542360e-03,  1.39652647e-03, ...,\n",
       "        -3.72194394e-04,  3.69547168e-04,  2.21411872e-04],\n",
       "       [-3.96784744e-04, -8.15429550e-04,  4.08606662e-04, ...,\n",
       "        -7.89205893e-04,  2.22029033e-04,  6.54699397e-05],\n",
       "       [-5.63650974e-05, -5.58513857e-04,  3.28945578e-04, ...,\n",
       "        -1.09319808e-03,  3.49186361e-04, -3.48822563e-04],\n",
       "       ...,\n",
       "       [-1.16920943e-04, -7.42095697e-04,  4.24565398e-04, ...,\n",
       "        -5.47854812e-04,  5.52729180e-04,  1.64238387e-04],\n",
       "       [-2.70342513e-04, -2.69885117e-04,  5.70217147e-04, ...,\n",
       "        -6.09003531e-04, -1.20140685e-04, -4.43551282e-04],\n",
       "       [-4.12418682e-04,  3.83455190e-05,  2.08073296e-04, ...,\n",
       "        -5.29231329e-04,  1.94052060e-04, -6.35411008e-04]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ZkNpyhAiHdTf"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1650327414384,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "oz7906mHLZag",
    "outputId": "e6f20502-4c1e-4f05-aa51-57ec37d89f87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_last = np.copy(y_pred)\n",
    "y_pred_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4527,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1650327470562,
     "user": {
      "displayName": "Wenxin Feng",
      "userId": "05139029976799982412"
     },
     "user_tz": 240
    },
    "id": "VExtwIp1LgGP",
    "outputId": "a2481117-feb4-4a4f-ae66-3dfb8959bfc3"
   },
   "outputs": [],
   "source": [
    "t = np.array(train_data['categories'].map({'earn':0,'acq':1}).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_data['categories'].map({'earn':0,'acq':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DEC {'ACC': 0.662, 'NMI': 0.034, 'ARI': 0.073}\n"
     ]
    }
   ],
   "source": [
    "print('Baseline DEC',three_metrics(t,list(y_pred_last)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
